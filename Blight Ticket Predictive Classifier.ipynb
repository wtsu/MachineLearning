{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective - Understand and Predicting Property Maintenance Fines\n",
    "\n",
    "This project is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "[Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition\n",
    "\n",
    "In the data provided, each row corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. \n",
    "\n",
    "I first analyze the data to understanding when and why a resident might fail to comply with a blight ticket. \n",
    "\n",
    "I then create a model using gradient boosted decision trees to predict whether a given blight ticket will be paid on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def blight_model():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #import the data sets\n",
    "    train = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")\n",
    "    test = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")\n",
    "    addresses = pd.read_csv('addresses.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #merge the addresses with the latitude/longitude file\n",
    "    train = pd.merge(train,addresses, how = 'inner', left_on = 'ticket_id', right_on='ticket_id')\n",
    "    test = pd.merge(test, addresses, how = 'inner', left_on = 'ticket_id', right_on='ticket_id')\n",
    "\n",
    "    #remove the parts of the train data where compliance = NaN\n",
    "    # subset: labels along other axis to consdier, e.g. if you are dropping rows these would be a list of columns to include\n",
    "    train = train.dropna(subset = ['compliance'])\n",
    "\n",
    "    #convert the compliance into labels for machine learning\n",
    "    train['compliance'] = train['compliance'].astype(int)\n",
    "\n",
    "    #create a dictionary of columns in the data that one wants to convert to category\n",
    "    convert_columns={'country': 'category',\n",
    "                     'non_us_str_code': 'category',\n",
    "                     'compliance': 'category',\n",
    "                     'state': 'category',\n",
    "                     'zip_code': 'category'\n",
    "                    }\n",
    "    #conver the dictionary of columns\n",
    "    for df in [test,train]:\n",
    "        for col, col_type in convert_columns.items():\n",
    "            if col in df:\n",
    "                if col_type == 'category':\n",
    "                    df[col] = df[col].replace(np.nan, \"NA\", regex = True).astype(col_type)\n",
    "                elif col_type == 'int':\n",
    "                    df[col] = df[col].replace(np.nan, 0, regex=True).astype(col_type)\n",
    "\n",
    "    #remove the unneeded columns from X sets\n",
    "    common_cols_to_drop = ['agency_name', 'inspector_name', 'mailing_address_str_number',\n",
    "                               'violator_name', 'violation_street_number', 'violation_street_name',\n",
    "                               'mailing_address_str_name', 'address', 'admin_fee', 'violation_zip_code',\n",
    "                               'state_fee', 'late_fee', 'ticket_issued_date', 'hearing_date', 'violation_description',\n",
    "                               'fine_amount', 'clean_up_cost', 'disposition', 'grafitti_status',\n",
    "                               'violation_code', 'city']\n",
    "    #create the column list to drop from training data\n",
    "    train_cols_to_drop = ['payment_status', 'payment_date', 'balance_due', 'payment_amount'] + common_cols_to_drop\n",
    "\n",
    "\n",
    "    #in the train data, drop to train_cols_to_drop\n",
    "    train = train.drop(train_cols_to_drop, axis=1).set_index('ticket_id')\n",
    "\n",
    "    # in the test data, drop common_cols_to_drop\n",
    "    test = test.drop(common_cols_to_drop, axis=1).set_index('ticket_id')\n",
    "\n",
    "    #get the train data labels\n",
    "    y_train = train['compliance']\n",
    "\n",
    "    #get the additional columns to drop from the training data\n",
    "    X_train_cols_to_drop = ['compliance', 'compliance_detail', 'collection_status']\n",
    "\n",
    "    #get the unlabeled training data\n",
    "    train = train.drop(X_train_cols_to_drop, axis = 1)\n",
    "\n",
    "    #get the category columns so that we can convert them to integers\n",
    "    cat_columns = train.select_dtypes(['category']).columns\n",
    "\n",
    "    #convert the category columns into categorical dummy variables\n",
    "    for df in [test, train]:\n",
    "        df[cat_columns] = df[cat_columns].apply(lambda x:x.cat.codes)\n",
    "\n",
    "    #create the X_train data\n",
    "    X_train = train.copy()\n",
    "\n",
    "    #use a gradient boosted ensemble of decision trees\n",
    "    grid_values = {'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 4, 5]}\n",
    "    clf = GradientBoostingClassifier(random_state = 0)\n",
    "    grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\n",
    "    grid_clf_auc.fit(X_train, y_train)\n",
    "    probs = grid_clf_auc.predict_proba(test)[:,1]\n",
    "    result = pd.Series(probs, index = test.index)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2856: DtypeWarning: Columns (11,12,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ticket_id\n",
       "284932    0.205176\n",
       "285362    0.094864\n",
       "285361    0.228263\n",
       "285338    0.205176\n",
       "285346    0.228263\n",
       "285345    0.308038\n",
       "285347    0.173571\n",
       "285342    0.608790\n",
       "285530    0.279054\n",
       "284989    0.048395\n",
       "285344    0.176957\n",
       "285343    0.094864\n",
       "285340    0.094864\n",
       "285341    0.176957\n",
       "285349    0.181394\n",
       "285348    0.145468\n",
       "284991    0.106330\n",
       "285532    0.119426\n",
       "285406    0.119426\n",
       "285001    0.119426\n",
       "285006    0.094864\n",
       "285405    0.065633\n",
       "285337    0.439720\n",
       "285496    0.176957\n",
       "285497    0.205176\n",
       "285378    0.039963\n",
       "285589    0.107934\n",
       "285585    0.205176\n",
       "285501    0.228263\n",
       "285581    0.092503\n",
       "            ...   \n",
       "376367    0.119426\n",
       "376366    0.112573\n",
       "376362    0.112573\n",
       "376363    0.205176\n",
       "376365    0.119426\n",
       "376364    0.112573\n",
       "376228    0.112573\n",
       "376265    0.112573\n",
       "376286    0.553783\n",
       "376320    0.112573\n",
       "376314    0.112573\n",
       "376327    0.553783\n",
       "376385    0.553783\n",
       "376435    0.119426\n",
       "376370    0.553783\n",
       "376434    0.176957\n",
       "376459    0.228263\n",
       "376478    0.042838\n",
       "376473    0.107662\n",
       "376484    0.084503\n",
       "376482    0.109218\n",
       "376480    0.109218\n",
       "376479    0.109218\n",
       "376481    0.109218\n",
       "376483    0.102882\n",
       "376496    0.094864\n",
       "376497    0.094864\n",
       "376499    0.228263\n",
       "376500    0.228263\n",
       "369851    0.486462\n",
       "Length: 61001, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blight_model()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
